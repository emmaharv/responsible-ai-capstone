{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9395c475",
   "metadata": {},
   "source": [
    "## DSC 180AB Data Science Capstone\n",
    "### Replication Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49e865",
   "metadata": {},
   "source": [
    "Team Members:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7f4e6-b2f0-4c7c-ab1f-9094a6339865",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "# Replication Project Part 03b, Discussion Section: Bias Mitigation Techniques\n",
    "## Refer to Replication Project Part 03a for the initial code and development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed5dfc-0e72-47b7-ba81-a36abe98b40c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3e294",
   "metadata": {},
   "source": [
    "## [5.](#Table-of-Contents) Bias Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267d787-ca80-486a-a183-703e54c58ab9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [5A.](#Table-of-Contents) Bias mitigation using pre-processing technique, Reweighing - AIF360 Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956822fc-281e-4f7e-ad3c-ebdacab682c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### [5B.](#Table-of-Contents) Prejudice Remover (in-processing bias mitigation) -  AIF360 Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff534f6-bbf7-4e77-ba99-15e24c43e784",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### [5C.](#Table-of-Contents) Bias mitigation using a technique of your own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6453816-0a2a-4ec2-9d7c-bb5e73141119",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64133381-263e-4569-bd34-fb8b3c5ba945",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 5 Discussion\n",
    "\n",
    "Use these questions to evaluate your models during bias-mitigation processes. \n",
    "\n",
    "### 5A. For **both** the logistic regression and random forest classifiers, please include visualizations the pre-processing results of your bias mitigation techniques. \n",
    "\n",
    "**In addition, for each Model + Bias mitigation technique, please write 1-2 SENTENCES explaining the following:**\n",
    "\n",
    "1. Describe the bias mitigation techniques applied (what stage? type? explain what that debiasing technique does?) \n",
    "2. Based on defintions and sources of bias we covered in class, what type of bias are we trying to mitigate in our models?\n",
    "3. Do both models exhibit fairness and maintain accuracy? List the fairness metrics you used to support this. What about model drift?\n",
    "4. For the classifier ‘high’ utilization in these models + pre-processing mitigation technique; would you recommend this for our use case as a \"fair\" classifier? Why or why not? Use previous questions and the slide 'Sources of Bias in AI and Health Data' from our Week-07 slides, to help you answer this. \n",
    "\n",
    "\n",
    "### 5B. For **both** the logistic regression and random forest classifiers, please include visualizations the post-processing results of your bias mitigation techniques. \n",
    "\n",
    "**In addition, for each Model + Bias mitigation technique, please write 1-2 SENTENCES explaining the following:**\n",
    "\n",
    "1. Describe the bias mitigation techniques applied (what stage? type? explain what that debiasing technique does?) \n",
    "2. Based on defintions and sources of bias we covered in class, what type of bias are we trying to mitigate in our models?\n",
    "3. Do both models exhibit fairness and maintain accuracy? List the fairness metrics you used to support this. What about model drift?\n",
    "4. For the classifier ‘high’ utilization in these models + post-processing mitigation technique; would you recommend this for our use case as a \"fair\" classifier? Why or why not? Use previous questions and the slide 'Sources of Bias in AI and Health Data' from our Week-07 slides, to help you answer this. \n",
    "\n",
    "### 5C. For **both** the logistic regression and random forest classifiers, please include visualizations for processing results of your bias mitigation techniques OF YOUR CHOICE. \n",
    "\n",
    "**In addition, for each Model + Bias mitigation technique of your choice, please write 1-2 SENTENCES explaining the following:**\n",
    "\n",
    "1. Describe the bias mitigation techniques applied (what stage? type? explain what that debiasing technique does?) \n",
    "2. Based on defintions and sources of bias we covered in class, what type of bias are we trying to mitigate in our models?\n",
    "3. Do both models exhibit fairness and maintain accuracy? List the fairness metrics you used to support this. What about model drift?\n",
    "4. For the classifier ‘high’ utilization in these models + post-processing mitigation technique; would you recommend this for our use case as a \"fair\" classifier? Why or why not? Use previous questions and the slide 'Sources of Bias in AI and Health Data' from our Week-07 slides, to help you answer this.\n",
    "\n",
    "## Section 5: Overall Discussion for Bias Mitigation, write 1-2 paragraphs for each question\n",
    "\n",
    "1. What factors must be considered during AI model-development and performance? How and where are they vulnerable to introducing bias?\n",
    "2. How and what should be measured to assess downstream impact of AI, and what factors should be used to audit for bias and clinical impact? \n",
    "3. Define what disparate impact is. How can the type, dimension, collection-method, and representation in data lead to bias and disparate impact in communities of concern? Use the MEPS codebooks, and websites to help you explain this. Refer to Center of Disease Control (CDC) [Health Equity Guiding Principles for Inclusive Communication](https://www.cdc.gov/healthcommunication/Health_Equity.html) for style recommendations when referring to affected groups. \n",
    "4. Where there any social factors overlooked when developing AI-targets or outcomes for the AIF360 exmaple? How could they delay access and quality of care to underserved populations?\n",
    "5. (a) Overall, if you were to select ONE (a) Model, and (b) Bias-mitigation technique - for this use case where you are recommending a 'Fair' classifier for flagging 'High' utilization, which would you chose and why? (b) How would it affect Non-White beneficiaries who could have risk factor predictors that could inform a model to prioritize additional care? How could it do the opposite? (c) Explain how you would justify fair-accuracy trade-off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389bce9-ad38-4b91-80f6-fdef6239856e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
